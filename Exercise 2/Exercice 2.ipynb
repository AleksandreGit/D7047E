{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Transfer Learning from ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "X_train, X_val= train_test_split(\n",
    "trainset, test_size=0.2, random_state=42)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(X_train, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "validationloader = torch.utils.data.DataLoader(X_val, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fe = models.alexnet(pretrained=True)\n",
    "\n",
    "# We don't calculate the gradient for the actual fully-connected layer\n",
    "for param in model_fe.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We get the number of input features of the fully-connected layer\n",
    "num_ftrs = model_fe.classifier[6].in_features\n",
    "\n",
    "# Adding a new fully connected linear layer\n",
    "model_fe.classifier[6] = nn.Sequential (\n",
    "    nn.Linear(num_ftrs, 300),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(300, 10),\n",
    "    nn.Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, modelVal, validationLoader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    finalLoss = 0\n",
    "    batchSize = 0 \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validationLoader, 0):\n",
    "            image, label = data\n",
    "            pred = model(image)\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, label)\n",
    "            finalLoss += loss       \n",
    "            batchSize = i\n",
    "    return finalLoss/batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    25] loss: 1.761 1 validation loss --> learning rate 0.001000000\n",
      "[1,    50] loss: 1.776 1 validation loss --> learning rate 0.001000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d702ccc6c992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m25\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m24\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# print every 25 mini-batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_fe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             print('[%d, %5d] loss: %.3f %d validation loss --> learning rate %.9f' %\n\u001b[0;32m     32\u001b[0m                   (epoch + 1, i + 1, running_loss / 25, val_loss, optimizer.param_groups[0]['lr']))\n",
      "\u001b[1;32m<ipython-input-14-cc03794ed5d9>\u001b[0m in \u001b[0;36mvalidate\u001b[1;34m(model, modelVal)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidationloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\alexnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 395\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fe.parameters(), lr=0.001)\n",
    "\n",
    "decayRate = 0.96\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #64, 3, 11, 11\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_fe(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        writer.add_scalar('Model1 Loss/train', loss.item(), epoch*625+i) \n",
    "        running_loss += loss.item()\n",
    "        if i % 25 == 24:    # print every 25 mini-batches\n",
    "            val_loss = validate(model_fe, 1)\n",
    "            print('[%d, %5d] loss: %.3f %.3f validation loss --> learning rate %.9f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 25, val_loss, optimizer.param_groups[0]['lr']))\n",
    "            writer.add_scalar('Model1 Validation Loss/train', val_loss, epoch*625+i) \n",
    "            running_loss = 0.0\n",
    "    scheduler.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = models.alexnet(pretrained=True)\n",
    "\n",
    "# We don't calculate the gradient for the actual fully-connected layer\n",
    "for param in model2.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# We get the number of input features of the fully-connected layer\n",
    "num_ftrs = model2.classifier[6].in_features\n",
    "\n",
    "# Adding a new fully connected linear layer\n",
    "model2.classifier[6] = nn.Linear(num_ftrs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "decayRate = 0.96\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #64, 3, 11, 11\n",
    "        # forward + backward + optimize\n",
    "        outputs = model2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        writer.add_scalar('Model2 Loss/train', loss.item(), epoch*625+i) \n",
    "        running_loss += loss.item()\n",
    "        if i % 25 == 24:    # print every 25 mini-batches\n",
    "            val_loss = validate(model2, 1)\n",
    "            print('[%d, %5d] loss: %.3f %.3f validation loss --> learning rate %.9f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 25, val_loss, optimizer.param_groups[0]['lr']))\n",
    "            writer.add_scalar('Model2 Validation Loss/train', val_loss, epoch*625+i) \n",
    "            running_loss = 0.0\n",
    "    scheduler.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Transfer Learning from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,activation_function):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 3),#28x28 -> 26x26 (no pad)\n",
    "            activation_function(),\n",
    "            nn.MaxPool2d(2, stride = 2), #2x2 maxpool (devide hight and with by 2 -> 13x13)\n",
    "            nn.Conv2d(6, 10, 3),#(no padding -> 11x11)\n",
    "            activation_function(),\n",
    "        )\n",
    "        self.linear_lay = nn.Sequential(\n",
    "            nn.Linear(1210, 300),# (11X11X10 (outchanels))\n",
    "            activation_function(),\n",
    "            nn.Linear(300, 50),\n",
    "            activation_function(),\n",
    "            nn.Linear(50,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0),-1)# this flattens X before linear layer\n",
    "        x = self.linear_lay(x) # bc this is our output layer. No activation here.\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data\\train_32x32.mat\n",
      "Using downloaded and verified file: ./data\\test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "# SVHN\n",
    "trainSVHN = torchvision.datasets.SVHN(root='./data', split='train',\n",
    "                                        download=True, transform=transform)\n",
    "X_trainSVHN, X_valSVHN= train_test_split(\n",
    "trainSVHN, test_size=0.2, random_state=42)\n",
    "\n",
    "trainSVHNLoader = torch.utils.data.DataLoader(X_trainSVHN, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "validationSVHNLoader = torch.utils.data.DataLoader(X_valSVHN, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testSVHN = torchvision.datasets.SVHN(root='./data', split='test',\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testSVHNLoader = torch.utils.data.DataLoader(testSVHN, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# MNIST                                           \n",
    "trainMNIST = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "X_trainMNIST, X_valMNIST= train_test_split(\n",
    "trainMNIST, test_size=0.2, random_state=42)\n",
    "\n",
    "trainMNISTLoader = torch.utils.data.DataLoader(X_trainMNIST, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "validationMNISTLoader = torch.utils.data.DataLoader(X_valMNIST, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testMNIST = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testMNISTLoader = torch.utils.data.DataLoader(testMNIST, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_digit(data):\n",
    "    data = data.view(28, 28)\n",
    "    plt.imshow(data, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "images, labels = next(iter(trainMNISTLoader))\n",
    "plot_digit(images[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   750] loss: 47.656 1.511 validation loss --> learning rate 0.000960000\n",
      "[2,   750] loss: 44.941 1.500 validation loss --> learning rate 0.000921600\n",
      "[3,   750] loss: 44.624 1.493 validation loss --> learning rate 0.000884736\n",
      "[4,   750] loss: 44.468 1.495 validation loss --> learning rate 0.000849347\n",
      "[5,   750] loss: 44.359 1.487 validation loss --> learning rate 0.000815373\n",
      "[6,   750] loss: 44.287 1.486 validation loss --> learning rate 0.000782758\n",
      "[7,   750] loss: 44.224 1.483 validation loss --> learning rate 0.000751447\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-c61641600912>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# loop over the dataset multiple times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainMNISTLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    912\u001b[0m             \u001b[1;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[1;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 914\u001b[1;33m             \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\multiprocessing\\reductions.py\u001b[0m in \u001b[0;36mreduce_storage\u001b[1;34m(storage)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mget_sharing_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file_system'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_share_filename_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mcache_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mrebuild\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrebuild_storage_filename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared_incref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "writer = SummaryWriter()\n",
    "modelMNIST = CNN(nn.LeakyReLU)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelMNIST.parameters(), lr=0.001)\n",
    "\n",
    "decayRate = 0.96\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainMNISTLoader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.view(-1, 1, 28, 28)\n",
    "        #64, 3, 11, 11\n",
    "        # forward + backward + optimize\n",
    "        outputs = modelMNIST(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        writer.add_scalar('MNIST Loss/train', loss.item(), epoch*750+i) \n",
    "        running_loss += loss.item()\n",
    "            \n",
    "    scheduler.step()\n",
    "    val_loss = validate(modelMNIST, 3, validationMNISTLoader)\n",
    "    print('[%d, %5d] loss: %.3f %.3f validation loss --> learning rate %.9f' %\n",
    "          (epoch + 1, i + 1, running_loss / 750, val_loss, optimizer.param_groups[0]['lr']))\n",
    "    writer.add_scalar('MNIST Validation Loss/train', val_loss, epoch) \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSVHN = modelMNIST\n",
    "\n",
    "# We don't calculate the gradient for the actual fully-connected layer\n",
    "for param in modelSVHN.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We get the number of input features of the fully-connected layer\n",
    "num_ftrs = modelSVHN.classifier[6].in_features\n",
    "\n",
    "# Adding a new fully connected linear layer\n",
    "modelSVHN.classifier[6] = nn.Sequential (\n",
    "    nn.Linear(num_ftrs, 300),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(300, 10),\n",
    "    nn.Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070 Super with Max-Q Design'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
